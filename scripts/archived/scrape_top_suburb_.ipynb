{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9599403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46a641bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0..1..2..3..4..5..6..7..8..9..10..11..12..13..14..15..16..17..18..19..20..21..22..23..24..25..26..27..28.."
     ]
    }
   ],
   "source": [
    "def get(url):\n",
    "    while True:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code == 200: \n",
    "            return response  \n",
    "\n",
    "def parse_row(row_data):\n",
    "    # Parse byte rank & compile full link\n",
    "    return [int(''.join(list(row_data[0])[1:])), row_data[1], \n",
    "            row_data[2], BASE_URL + row_data[3]]\n",
    "\n",
    "def get_page(page):\n",
    "    # Retrieves the suburb ranking table of a page\n",
    "    page_data = []\n",
    "    bs_obj = BeautifulSoup(get(SUBURB_SEARCH_URL % page).content, \"html.parser\")\n",
    "    # Find suburbs table & rowdata object\n",
    "    tab = bs_obj.find(\"table\", {\"cellspacing\": \"0\", \"cellpadding\": \"0\", \"width\": \"100%\", \"style\": None})\n",
    "    rows = tab.findAll(\"tr\")\n",
    "    for row in rows[1:]: # First row is headers\n",
    "        row_data_ = row.findAll(\"td\")\n",
    "        row_data = [c.text for c in row_data_] # Unpack rank, suburb, price\n",
    "        # Append link suffix\n",
    "        suffix = row_data_[1].find(\"a\")['href']\n",
    "        row_data += [suffix]\n",
    "        # Parse row data\n",
    "        parsed_row_data = parse_row(row_data)\n",
    "        page_data.append(parsed_row_data)\n",
    "    return page_data\n",
    "\n",
    "def get_suburbs(verbose=True):\n",
    "    # Retrieves all pages of the suburbs rankings\n",
    "    df = pd.DataFrame()\n",
    "    page = 0\n",
    "    while True:\n",
    "        if verbose: print(page, end='..', flush=True)\n",
    "        page_data = get_page(page)\n",
    "        if len(page_data) == 0: break # Beyond the last page, stop\n",
    "        page_df = pd.DataFrame(page_data, columns=TAB_COLS)\n",
    "        page_df['page'] = page\n",
    "        df = pd.concat([df, page_df])\n",
    "        page += 1\n",
    "    return df\n",
    "\n",
    "def get_suburb_sales_data(ingredients):\n",
    "    # Get \"Sales Data\" table from suburb profile page    \n",
    "    soup = BeautifulSoup(ingredients, \"html.parser\")\n",
    "    heading = soup.find(\"table\", {\"style\": \"font-size:18px\"}) # Suburb Post code & (distnace to CBD link)\n",
    "    sub_pc = heading.find('b').text\n",
    "    sales_table = soup.find(\"table\", {\"style\": \"font-size:13px\", \"cellspacing\": 10})\n",
    "    \n",
    "    table_data = sales_table.findAll(\"td\")\n",
    "    assert table_data[0].text == \"Sales Data\"\n",
    "    cell_data = table_data[2:]\n",
    "    \n",
    "    sales_data = []\n",
    "    for i in range(0, len(cell_data), 3):    \n",
    "        sales_data.append([cell_data[i].text, cell_data[i+2].text])\n",
    "    sales_df = pd.DataFrame(sales_data, columns=['key','value'])\n",
    "    return sales_df\n",
    "\n",
    "def get_suburbs_sales_data(verbose=True):\n",
    "    sales_df = pd.DataFrame()\n",
    "    errors = []\n",
    "    if verbose: print(len(df['link'].values))\n",
    "    for i, link in enumerate(df['link'].values):\n",
    "        if verbose: print(i, end='..')\n",
    "        try:\n",
    "            sub_sales_df = get_suburb_sales_data(get(link).content)\n",
    "            sub_sales_df['link'] = link\n",
    "            sales_df = pd.concat([sales_df, sub_sales_df])\n",
    "        except AttributeError:\n",
    "            # No \"Sales Data\" table\n",
    "            errors.append(link)\n",
    "    if verbose: print(len(errors), \"errors\")\n",
    "    return sales_df, errors\n",
    "\n",
    "async def async_get(client, url):\n",
    "    async with client.get(url) as response:\n",
    "        assert response.status == 200        \n",
    "        return await response.read(), url\n",
    "\n",
    "async def async_get_links_(client, links):\n",
    "    resps = await asyncio.wait([async_get(client, link) for link in links])\n",
    "    return resps\n",
    "\n",
    "def async_get_links(links):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    client = aiohttp.ClientSession(loop=loop)\n",
    "    resps = loop.run_until_complete(async_get_links_(client, links))\n",
    "    loop.close()\n",
    "    return resps\n",
    "\n",
    "def async_get_suburbs_sales_data(verbose=True):\n",
    "    resps_ = async_get_links(df['link'].values)\n",
    "    resps = [x.result() for x in list(list(resps_)[0])]\n",
    "    sales_df = pd.DataFrame()\n",
    "    errors = []\n",
    "    for i, (resp, link) in enumerate(resps):\n",
    "        try:\n",
    "            print(i, end='..', flush=True)\n",
    "            sub_sales_df = get_suburb_sales_data(resp)\n",
    "            sub_sales_df['link'] = link\n",
    "            sales_df = pd.concat([sales_df, sub_sales_df])\n",
    "        except AttributeError:\n",
    "            # No \"Sales Data\" table\n",
    "            errors.append(link)\n",
    "    if verbose: print(len(errors), \"errors\")\n",
    "    return sales_df, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "\n",
    "name_list = [ 'Housing_Turnover','Median_household_income', 'Occupation', 'Education', 'Type_of_Dwelling_Flat',\n",
    "              'Type_of_Dwelling_Separate_house', 'Type_of_Dwelling_Separate_house',  'Nature_of_Occupancy_Fully_Owned',\n",
    "             'Nature_of_Occupancy_Rented', 'Nature_of_Occupancy_Purchasing', 'HomePrice'\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3af84cd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to scrape Housing_Turnover\n",
      "0.."
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart to scrape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnames[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m SUBURB_SEARCH_URL \u001b[38;5;241m=\u001b[39m BASE_URL \u001b[38;5;241m+\u001b[39m body \u001b[38;5;241m+\u001b[39m ENDING\n\u001b[1;32m---> 32\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mget_suburbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/top_rent_suburbs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnames[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend to scrape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnames[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36mget_suburbs\u001b[1;34m(verbose)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(page, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 43\u001b[0m     page_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(page_data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m \u001b[38;5;66;03m# Beyond the last page, stop\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     page_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(page_data, columns\u001b[38;5;241m=\u001b[39mTAB_COLS)\n",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36mget_page\u001b[1;34m(page)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Find suburbs table & rowdata object\u001b[39;00m\n\u001b[0;32m     24\u001b[0m tab \u001b[38;5;241m=\u001b[39m bs_obj\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcellspacing\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcellpadding\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m})\n\u001b[1;32m---> 25\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mtab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindAll\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows[\u001b[38;5;241m1\u001b[39m:]: \u001b[38;5;66;03m# First row is headers\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     row_data_ \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfindAll(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "BASE_URL = 'http://house.speakingsame.com/suburbtop.php?sta=vic&cat='\n",
    "\n",
    "TAB_COLS = ['rank', 'suburb', 'value', 'link']\n",
    "\n",
    "BODIES = ['Housing+Turnover&name=Last+12+months', \n",
    "         'Median+household+income&name=Weekly+income',\n",
    "         'Occupation&name=Professionals',\n",
    "         'Education&name=University+or+other+Tertiary+Institution',\n",
    "         'Type+of+Dwelling&name=Flat',\n",
    "         'Type+of+Dwelling&name=Separate+house',\n",
    "         'Nature+of+Occupancy&name=Fully+Owned',\n",
    "         'Nature+of+Occupancy&name=Rented',\n",
    "         'Nature+of+Occupancy&name=Purchasing',\n",
    "         'HomePrice&name='\n",
    "        ]\n",
    "\n",
    "ENDING = '&page=%s'\n",
    "\n",
    "# SUBURB_SEARCH_URL = 'http://house.speakingsame.com/suburbtop.php?sta=vic&cat=Housing+Turnover&name=Last+12+months'\n",
    "\n",
    "names = [ 'Housing_Turnover','Median_household_income', 'Occupation', 'Education', 'Type_of_Dwelling_Flat',\n",
    "              'Type_of_Dwelling_Separate_house', 'Type_of_Dwelling_Separate_house',  'Nature_of_Occupancy_Fully_Owned',\n",
    "             'Nature_of_Occupancy_Rented', 'Nature_of_Occupancy_Purchasing', 'HomePrice'\n",
    "            ]\n",
    "\n",
    "i = 0\n",
    "for body in BODIES:\n",
    "    \n",
    "    print(f'start to scrape {names[i]}')\n",
    "    SUBURB_SEARCH_URL = BASE_URL + body + ENDING\n",
    "    \n",
    "    df = get_suburbs()\n",
    "    df.to_csv(f'../data/raw/top_rent_suburbs_{names[i]}.csv', index=False)\n",
    "    \n",
    "    \n",
    "    print(f'end to scrape {names[i]}')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19fa9890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to scrape Housing_Turnover\n",
      "0..1..2..3..4..5..6..7..8..9..10..11..12..13..14..15..16..17..18..19..20..21..22..23..24..25..26..27..28..end to scrape Housing_Turnover\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "i = 0\n",
    "\n",
    "print(f'start to scrape {names[i]}')\n",
    "\n",
    "SUBURB_SEARCH_URL = BASE_URL + BODIES[i]+ ENDING\n",
    "\n",
    "df = get_suburbs()\n",
    "df.to_csv(f'../data/raw/top_rent_suburbs_{names[i]}.csv', index=False)\n",
    "    \n",
    "print(f'end to scrape {names[i]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1989a10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to scrape Median_household_income\n",
      "0..1..2..3..4..5..6..7..8..9..10..11..12..13..14..end to scrape Median_household_income\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "i = 1\n",
    "\n",
    "print(f'start to scrape {names[i]}')\n",
    "\n",
    "SUBURB_SEARCH_URL = BASE_URL + BODIES[i]+ ENDING\n",
    "\n",
    "df = get_suburbs()\n",
    "df.to_csv(f'../data/raw/top_rent_suburbs_{names[i]}.csv', index=False)\n",
    "    \n",
    "print(f'end to scrape {names[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b22990d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to scrape Occupation\n",
      "0..1..2..3..4..5..6..7..8..9..10..11..12..end to scrape Occupation\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "\n",
    "i = 2\n",
    "print(f'start to scrape {names[i]}')\n",
    "\n",
    "SUBURB_SEARCH_URL = BASE_URL + BODIES[i]+ ENDING\n",
    "\n",
    "df = get_suburbs()\n",
    "df.to_csv(f'../data/raw/top_rent_suburbs_{names[i]}.csv', index=False)\n",
    "    \n",
    "print(f'end to scrape {names[i]}')\n",
    "\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6b97326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to scrape Education\n",
      "0..1..2..3..4..5..6..7..8..9..10..11..end to scrape Education\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "\n",
    "i = 3\n",
    "print(f'start to scrape {names[i]}')\n",
    "\n",
    "SUBURB_SEARCH_URL = BASE_URL + BODIES[i]+ ENDING\n",
    "\n",
    "df = get_suburbs()\n",
    "df.to_csv(f'../data/raw/top_rent_suburbs_{names[i]}.csv', index=False)\n",
    "    \n",
    "print(f'end to scrape {names[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06dd0658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to scrape Type_of_Dwelling_Flat\n",
      "0..1..2..3..4..5..6..7..8..9..10..11..12..13..end to scrape Type_of_Dwelling_Flat\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "\n",
    "i = 4\n",
    "print(f'start to scrape {names[i]}')\n",
    "\n",
    "SUBURB_SEARCH_URL = BASE_URL + BODIES[i]+ ENDING\n",
    "\n",
    "df = get_suburbs()\n",
    "df.to_csv(f'../data/raw/top_rent_suburbs_{names[i]}.csv', index=False)\n",
    "    \n",
    "print(f'end to scrape {names[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8584af46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to scrape Type_of_Dwelling_Separate_house\n",
      "0..1..2..3..4..5..6..7..8..9..10..11..12..13..14..end to scrape Type_of_Dwelling_Separate_house\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "\n",
    "i = 5\n",
    "print(f'start to scrape {names[i]}')\n",
    "\n",
    "SUBURB_SEARCH_URL = BASE_URL + BODIES[i]+ ENDING\n",
    "\n",
    "df = get_suburbs()\n",
    "df.to_csv(f'../data/raw/top_rent_suburbs_{names[i]}.csv', index=False)\n",
    "    \n",
    "print(f'end to scrape {names[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5346225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to scrape Type_of_Dwelling_Separate_house\n",
      "0..1..2..3..4..5..6..7..8..9..10..11..12..13..14..end to scrape Type_of_Dwelling_Separate_house\n"
     ]
    }
   ],
   "source": [
    "# 7\n",
    "\n",
    "i = 6\n",
    "print(f'start to scrape {names[i]}')\n",
    "\n",
    "SUBURB_SEARCH_URL = BASE_URL + BODIES[i]+ ENDING\n",
    "\n",
    "df = get_suburbs()\n",
    "df.to_csv(f'../data/raw/top_rent_suburbs_{names[i]}.csv', index=False)\n",
    "    \n",
    "print(f'end to scrape {names[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "22f8b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to scrape Nature_of_Occupancy_Fully_Owned\n",
      "0..1..2..3..4..5..6..7..8..9..10..11..12..13..14..end to scrape Nature_of_Occupancy_Fully_Owned\n"
     ]
    }
   ],
   "source": [
    "# 8\n",
    "\n",
    "i = 7\n",
    "print(f'start to scrape {names[i]}')\n",
    "\n",
    "SUBURB_SEARCH_URL = BASE_URL + BODIES[i]+ ENDING\n",
    "\n",
    "df = get_suburbs()\n",
    "df.to_csv(f'../data/raw/top_rent_suburbs_{names[i]}.csv', index=False)\n",
    "    \n",
    "print(f'end to scrape {names[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4a28f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to scrape Nature_of_Occupancy_Rented\n",
      "0..1..2..3..4..5..6..7..8..9..10..11..12..13..14..end to scrape Nature_of_Occupancy_Rented\n"
     ]
    }
   ],
   "source": [
    "# 9\n",
    "\n",
    "i=8\n",
    "print(f'start to scrape {names[i]}')\n",
    "\n",
    "SUBURB_SEARCH_URL = BASE_URL + BODIES[i]+ ENDING\n",
    "\n",
    "df = get_suburbs()\n",
    "df.to_csv(f'../data/raw/top_rent_suburbs_{names[i]}.csv', index=False)\n",
    "    \n",
    "print(f'end to scrape {names[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bc51329b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to scrape Nature_of_Occupancy_Purchasing\n",
      "0..1..2..3..4..5..6..7..8..9..10..11..12..13..14..15..16..17..18..19..20..21..22..23..24..25..26..end to scrape Nature_of_Occupancy_Purchasing\n"
     ]
    }
   ],
   "source": [
    "# 10\n",
    "\n",
    "i=9\n",
    "print(f'start to scrape {names[i]}')\n",
    "\n",
    "SUBURB_SEARCH_URL = BASE_URL + BODIES[i]+ ENDING\n",
    "\n",
    "df = get_suburbs()\n",
    "df.to_csv(f'../data/raw/top_rent_suburbs_{names[i]}.csv', index=False)\n",
    "    \n",
    "print(f'end to scrape {names[i]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
